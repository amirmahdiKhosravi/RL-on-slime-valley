{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37d5b88",
   "metadata": {},
   "source": [
    "# **Code for first presentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "535f5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "from gym import wrappers\n",
    "import time\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "env_to_wrap = gym.make(\"SlimeVolley-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66708eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 553 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(env_to_wrap, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%S') + '/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ab433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'UP', 'RIGHT', 'LEFT', 'UPRIGHT', 'UPLEFT']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c9cda",
   "metadata": {},
   "source": [
    "# **Second part of the project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "661f1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.15 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (3.19.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.13.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.42.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: h5py in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (58.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
      "Requirement already satisfied: cached-property in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "#python version must be 3.7\n",
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375e1a66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines[mpi]==2.10.2 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (2.10.2)\n",
      "Requirement already satisfied: scipy in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (1.7.3)\n",
      "Requirement already satisfied: opencv-python in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (4.5.4.60)\n",
      "Requirement already satisfied: pandas in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (1.3.4)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (1.21.4)\n",
      "Requirement already satisfied: joblib in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (1.1.0)\n",
      "Requirement already satisfied: matplotlib in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (3.5.0)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (0.21.0)\n",
      "Requirement already satisfied: mpi4py in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (4.8.2)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (1.5.21)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (0.7.3)\n",
      "Requirement already satisfied: importlib-resources in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from ale-py~=0.7.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (4.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (4.28.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (6.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines[mpi]==2.10.2) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->stable-baselines[mpi]==2.10.2) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->stable-baselines[mpi]==2.10.2) (58.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from pandas->stable-baselines[mpi]==2.10.2) (2021.3)\n",
      "Requirement already satisfied: slimevolleygym in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from slimevolleygym) (1.21.4)\n",
      "Requirement already satisfied: gym>=0.9.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from slimevolleygym) (0.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym>=0.9.4->slimevolleygym) (4.8.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym>=0.9.4->slimevolleygym) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym>=0.9.4->slimevolleygym) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym>=0.9.4->slimevolleygym) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines[mpi]==2.10.2\n",
    "!pip install slimevolleygym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed993e",
   "metadata": {},
   "source": [
    "### PPO1 algorithm directly trained against Built-in expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed472aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "from stable_baselines import PPO1\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.bench import Monitor\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ad4e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 768\n",
      "Best mean reward: -inf - Last mean reward per episode: -5.00\n",
      "Saving new best model to ./logs/best_model\n",
      "Num timesteps: 1792\n",
      "Best mean reward: -5.00 - Last mean reward per episode: -4.67\n",
      "Saving new best model to ./logs/best_model\n",
      "Num timesteps: 2816\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.80\n",
      "Num timesteps: 3840\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 4864\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.89\n",
      "Num timesteps: 5888\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.91\n",
      "Num timesteps: 6912\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 7936\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n",
      "Num timesteps: 8960\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.73\n",
      "Num timesteps: 9984\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.76\n",
      "Num timesteps: 10752\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.78\n",
      "Num timesteps: 11776\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.75\n",
      "Num timesteps: 12800\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.77\n",
      "Num timesteps: 13824\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.78\n",
      "Num timesteps: 14848\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.75\n",
      "Num timesteps: 15872\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.73\n",
      "Num timesteps: 16896\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.75\n",
      "Num timesteps: 17920\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.76\n",
      "Num timesteps: 18944\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.77\n",
      "Num timesteps: 19968\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.79\n",
      "Num timesteps: 20992\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.80\n",
      "Num timesteps: 21760\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.81\n",
      "Num timesteps: 22784\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.82\n",
      "Num timesteps: 23808\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.83\n",
      "Num timesteps: 24832\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.83\n",
      "Num timesteps: 25856\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.84\n",
      "Num timesteps: 26880\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.84\n",
      "Num timesteps: 27904\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.85\n",
      "Num timesteps: 28928\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.83\n",
      "Num timesteps: 29952\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.82\n",
      "Num timesteps: 30976\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.82\n",
      "Num timesteps: 31744\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.83\n",
      "Num timesteps: 32768\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.82\n",
      "Num timesteps: 33792\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.82\n",
      "Num timesteps: 34816\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.83\n",
      "Num timesteps: 35840\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.83\n",
      "Num timesteps: 36864\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.84\n",
      "Num timesteps: 37888\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.84\n",
      "Num timesteps: 38912\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.84\n",
      "Num timesteps: 39936\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.85\n",
      "Num timesteps: 40960\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.85\n",
      "Num timesteps: 41984\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.85\n",
      "Num timesteps: 42752\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 43776\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 44800\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 45824\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 46848\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 47872\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 48896\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 49920\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.88\n",
      "Num timesteps: 50944\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 51968\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 52992\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 53760\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 54784\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.88\n",
      "Num timesteps: 55808\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.88\n",
      "Num timesteps: 56832\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.88\n",
      "Num timesteps: 57856\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.88\n",
      "Num timesteps: 58880\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 59904\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 60928\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 61952\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 62976\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 63744\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 64768\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 65792\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 66816\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 67840\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 68864\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.87\n",
      "Num timesteps: 69888\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.86\n",
      "Num timesteps: 70912\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.89\n",
      "Num timesteps: 71936\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.89\n",
      "Num timesteps: 72960\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.90\n",
      "Num timesteps: 73984\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.90\n",
      "Num timesteps: 74752\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.90\n",
      "Num timesteps: 75776\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.91\n",
      "Num timesteps: 76800\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 77824\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 78848\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 79872\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 80896\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 81920\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 82944\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 83968\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 84992\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 85760\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 86784\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 87808\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 88832\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.91\n",
      "Num timesteps: 89856\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.91\n",
      "Num timesteps: 90880\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.91\n",
      "Num timesteps: 91904\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 92928\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 93952\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.92\n",
      "Num timesteps: 94976\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n",
      "Num timesteps: 95744\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n",
      "Num timesteps: 96768\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n",
      "Num timesteps: 97792\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 98816\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n",
      "Num timesteps: 99840\n",
      "Best mean reward: -4.67 - Last mean reward per episode: -4.93\n",
      "mean_reward:-5.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SlimeVolley-v0')\n",
    "env = Monitor(env, './logs/')\n",
    "eval_env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir='./logs/')\n",
    "\"\"\"eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\"\"\"\n",
    "\n",
    "model = PPO1('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=100000, callback=eval_callback)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=1)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffff48",
   "metadata": {},
   "source": [
    "### chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23fc2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b29e7455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAACICAYAAAD9AgFjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWR0lEQVR4nO3deZhdVZ3u8e9LgATCEGJAIJABhNYIGmIJOHUrREBEQ9/WC4gKDs3l2ra0ijhgM4iNdGuD4XG4pHECBBVEwQHtBLHBgSGFgQSUJAwBAmkChBCGZpC3/9ir6EPdGnZV6tSpU3k/z3Oe2mvvdfb+nbNrV35Ze+21ZJuIiIiIdrRRqwOIiIiIGKwkMhEREdG2kshERERE20oiExEREW0riUxERES0rSQyERER0baSyEREDANJlvSSVscRMdokkYkYxSTdJelJSeskPSLpd5KOlbRRQ51vS3q61FknaYmkL0jauqHO0ZL+LOkxSY9KWiTpkIbtW0o6sxzvcUl3S7pE0j69xDWt/MO+cXO/gf4lwYhob0lkIka/t9neEpgKnAF8EvhGtzr/UupsC7wP2Bf4raTxDXV+b3sLYEJ5/w8kbSNpLPArYE/gEGAr4GXA94C3NO1TraeRkETV0S5xRrRKEpmIDYTttbYvBw4DjpK0Rw91/sv2DcDbgRdRJTXd6zwHfBPYDNgVeA+wE3Co7SW2/2z7cduX2D5loHGWVp3ZDeVTJF1Qlrtaco4qrT4PSjqxoe7ekn5fWp/ul/QVSZs2bLekv5O0DFjWTxy7SvqVpIfKcb4raUK3OI+XdLOktZK+L2lcw/ZPlBjuk/T+bvseK+lL5TP8p6T/J2mzsu2Nku6V9ElJq4BvDfQ7jNiQJJGJ2MDYvh64F3hDH3XWAfN7qlNaCD4IPEaVDMwGfmn78aYE3LPXA38B7A+cJOllZf2fgY8Ck4DXlO0f6vbeQ4F9gBn9HEPAF4AdqVqYdgZO6VbnfwMHAdOBVwBHA0g6CDgeeDOwG9V31OgMYHdgJvASYDJwUsP27YGJVK1ox/QTZ8QGLYlMxIbpPqp/KAdSZ19JjwCrgCOAv7a9lippWNVVSdLM0iLyqKTbhjbs551q+0nbNwE3Aa8EsN1p+1rbz9q+CzgH+Ktu7/2C7YdtP9nXAWwvtz3f9lO2VwNn9rCvs23fZ/th4CdUiQlUCc63SgvV4zQkQJJElZx8tMSxDjgdOLxhv88BJ5dj9xlnxIYu914jNkyTgYcHWOda26/vod5DwA5dBduLgAnl9tC56xlnb1Y1LD8BbAEgaXeqhKMD2Jzqb1xnt/feU+cAkl4MzKVqldqS6j9+a/qJY8eyvGO3465oWN62xNZZ5TTV4YAxDXVW2/6vOnFGbOjSIhOxgZH0aqok5Td91NmC6nbINTV2eSVwQLeOwevjcap/6LtsP4D3fh34E7Cb7a2Az1AlCY1cc1+nl7p7ln29u4d99eZ+qltRXaY0LD8IPAm83PaE8tq6dKQeaIwRG7wkMhEbCElblUemvwdcYHtxD3XGSnoV8GOq1oc6HU3Po/qH+0eS9pA0pnR67ajx3rGSxjW8NgIWAYdL2kRSB/COWh+wsiXwKPCYpJcC/7fm+zbtFseYsq/HgLWSJgOfGEAcPwCOljRD0ubAyV0bSmfpfwPOkrQdgKTJkg4cwP4jokgiEzH6/UTSOqpbKidS3Xrp/jTSCaXOQ1SJSSfw2jodeMstkDcBtwI/o0okbgNeTdVXpC+PUbVOdL32A/6R6mmoNcCpwIX9f8TnHQ+8C1hHlSx8v+b7bukWx/vKsWcBa6k+16V1g7B9BfBlqsfSl5efjT5Z1l8r6VFgAVXn5YgYINlpwYyIiIj2lBaZiIiIaFsDSmQkbSRpq2YFExERETEQ/SYyki4snQTHA0uAWyUNpNNbRERERFPUaZGZYftRqtEwr6AawfI9zQwqIiIioo46A+JtImkTqkTmK7afkTRiewhPmjTJ06ZNa3UYERERMUQ6OzsftL1tT9vqJDLnAHdRDQN+taSpVI9XrjdJHwe+BGxr+8Eeth8FfLYUP2/7O/3tc9q0aSxcuHAowouIiIgRQNKK3rb1m8jYPhs4u2HVCklvGoKgdgYOAO7uZftEqkGkOqhGueyUdLnt7kOER0RExAaq10RG0sf6ee+Z63nss4ATgMt62X4gML9Mxoak+VSzzF60nseNAehcsYa5C5Zy3OzdedXUbVodTo/aIcaIiGiOvjr7blleHVTDfE8ur2OpRrscNElzgJVl5treTOaFk7vdW9b1tL9jJC2UtHD16tXrE1p0M3fBUq5e9iBzFyxtdSi9aocYIyKiOXptkbF9KoCkq4FZZap5JJ1CNVx3nyQtoOfJ3k6kmsjtgEHE21us84B5AB0dHSO2I3I7Om727i/4ORK1Q4wREdEcdTr7vhh4uqH8dFnXJ9uze1ovaU+qR7hvKlPY7wTcKGlv26saqq4E3thQ3gn4dY14Ywi9auo2nPeBfVodRp/aIcaIiGiOOonMecD1kn5UyocC3x7sAcuMu9t1lSXdBXT08NTSL4HTJXV1ejgA+PRgjxsRERGjT5+JjKomk/OoBsJ7Q1n9Ptt/aEYwkjqAY21/0PbDkk4DbiibP9fV8TciIiICasx+LWmx7T2HKZ711tHR4YwjExERMXpI6rTd0dO2OlMU3Cjp1UMcU0RERMR6q9NHZh/gyDKq3uOAANt+RVMji4iIiOhHnUTmwKZHERERETEIdaYoWAEgaTtgXNMjioiIiKip3z4ykt4uaRlwJ/AfVBNIXtHkuCIiIiL6Vaez72nAvsBS29OB/YFrmxpVRERERA11EplnbD8EbCRpI9tXUc2/FBEREdFSdTr7PiJpC+Bq4LuSHqB6eikiIiKipeq0yMwBngA+CvwCuB14WzODioiIiKijTovM4cDVtpcB32lyPBERERG11UlkpgDnSJoOLKS6xXSN7UXNDCwiIiKiP/3eWrJ9su39gBnANcAngM5mBxYRERHRn35bZCR9FngdsAXwB+B4qoQmIiIioqXq3Fr6X8CzwM+oBsT7ve2nmhpVRERERA11bi3NAmYD1wNvBhZL+k2zA4uIiIjoT51bS3sAbwD+imogvHvIraWIiIgYAercWjqDKnE5G7jB9jPNDSkiIiKinjqzXx8iaTNgSpKYiIiIGEnqzH79NmAR1ai+SJop6fImxxURERHRrzpTFJwC7A08AlAGwpvetIgiIiIiaqo7+/XabuvcjGAiIiIiBqJOZ99bJL0LGCNpN+AjwO+aG1ZERERE/+q0yPw98HLgKeAiYC1wXDODioiIiKijzoB4T9g+0farbXcA5wNfaX5oEREREX3rNZGR9ApJ/y5piaTPS9pB0g+BK4Fbhy/EiIiIiJ711SLzb8CFwN8AD1I9gn078BLbZw3FwSV9XJIlTepl+58lLSqvPPIdERERL9BXZ9+xtr9dlm+T9BHbJwzVgSXtDBwA3N1HtSdtzxyqY0ZERMTo0lciM07SXoBK+anGsu0b1/PYZwEnAJet534iIiJiA9VXInM/cGZDeVVD2cB+gz2opDnASts3Seqr6jhJC4FngTNs/7iX/R0DHAMwZcqUwYYVERERbabXRMb2m9Znx5IWANv3sOlE4DNUt5X6M9X2Skm7AL+StNj27T3EOg+YB9DR0ZHB+iIiIjYQdQbEGxTbs3taL2lPqikOulpjdgJulLS37VXd9rGy/LxD0q+Bvag6HEdERETUGhBvSNlebHs729NsTwPuBWZ1T2IkbSNpbFmeBLyOPPYdERERDYY9kemLpA5J55biy4CFkm4CrqLqI5NEJiIiIp7X760lVfd/jgR2sf05SVOA7W1fPxQBlFaZruWFwAfL8u+APYfiGBERETE61WmR+RrwGuCIUl4HfLVpEUVERETUVKez7z62Z0n6A4DtNZI2bXJcEREREf2q0yLzjKQxVGPHIGlb4LmmRhURERFRQ51E5mzgR8B2kv4J+A1welOjioiIiKih31tLtr8rqRPYn2p6gkNt/7HpkUVERET0o9dERtLEhuIDwEWN22w/3MzAIiIiIvrTV4tMJ1W/GAFTgDVleQLVjNXTmx1cRERERF967SNje7rtXYAFwNtsT7L9IuAQ4N+HK8CIiIiI3tTp7Luv7Z93FWxfAby2eSFFRERE1FNnHJn7JH0WuKCUjwTua15IEREREfXUaZE5AtiW6hHsHwHb8T+j/EZERES0TJ3Hrx8GjpO0ZVX0Y80PKyIiIqJ//bbISNqzTE+wBLhFUqekPZofWkRERETf6txaOgf4mO2ptqcCHwfmNTesiIiIiP7VSWTG276qq2D718D4pkUUERERUVOdp5bukPSPwPml/G7gjuaFFBEREVFPnRaZ91M9tXRpeU0q6yIiIiJaqs5TS2uAjwBIGkN1q+nRZgcWERER0Z86Ty1dKGkrSeOBxcCtkj7R/NAiIiIi+lbn1tKM0gJzKHAF1WSR72lmUBERERF11ElkNpG0CVUic7ntZ6hmxY6IiIhoqbrjyNxF9cj11ZKmAukjExERES1Xp7Pv2cDZDatWSHpT80KKiIiIqKfXREbSu21fIOljvVQ5s0kxRURERNTSV4tM1+i9Ww5HIBERERED1WsiY/uc8vPUoT6opFOAvwVWl1Wfsf3zHuodBMwFxgDn2j5jqGOJiIiI9lVnHJldJP1E0mpJD0i6TNIuQ3Dss2zPLK+ekpgxwFeBtwAzgCMkzRiC40ZERMQoUeeppQuBHwA7ADsCFwMXNTOoYm9gue07bD8NfA+YMwzH7VXnijW89xvX0bliTVvtO5pjuM7ZSPrdGEmx9CexDv8xWnW8djrXg9HszzfSv7/OFWvYeOLk3XrbXieR2dz2+bafLa8LgHFDENuHJd0s6ZuStulh+2TgnobyvWXd/0fSMZIWSlq4evXqnqoMibkLlnL1sgeZu2BpW+07mmO4ztlI+t0YSbH0J7EO/zFadbx2OteD0ezPN9K/v7kLlrLRpptt1dv2OrNfXyHpU1QtIgYOA34uaSKA7Yd7epOkBcD2PWw6Efg6cFrZ32nAv7IeE1HangfMA+jo6GjaYH3Hzd79BT/bZd/RHMN1zkbS78ZIiqU/iXX4j9Gq47XTuR6MZn++kf79HTd7dy464clex6+T3fe/+5Lu7GOzba9XfxlJ04Cf2t6j2/rXAKfYPrCUP10O+IW+9tfR0eGFCxeuT0gRERExgkjqtN3R07Y6A+JNb0JAO9i+vxT/GljSQ7UbgN0kTQdWAocD7xrqWCIiIqJ99doiI+kE2/9Slt9p++KGbafb/sygDyqdD8ykurV0F/B/bN8vaUeqx6wPLvUOBr5M9fj1N23/U419rwNuG2xs0TSTgAdbHUS8QM7JyJNzMjLlvLTeVNvb9rShr0TmRtuzui/3VB5JJC3srfkpWifnZeTJORl5ck5GppyXka2vp5bUy3JP5YiIiIhh11ci416WeypHREREDLu+Ovu+UtKjVK0vm5VlSnkoxpFplnmtDiB6lPMy8uScjDw5JyNTzssI1u/j1xEREREjVZ2RfSMiIiJGpCQyERER0bZGVSIj6SBJt0laXqZViCaRtLOkqyTdKukWSceV9RMlzZe0rPzcpqyXpLPLublZUuPj/EeV+sskHdWqzzRaSBoj6Q+SflrK0yVdV77770vatKwfW8rLy/ZpDfv4dFl/m6QDW/RRRgVJEyRdIulPkv4o6TW5TlpP0kfL364lki6SNC7XSnsaNYmMpDHAV4G3ADOAIyTNaG1Uo9qzwMdtzwD2Bf6ufN+fAq60vRtwZSlDdV52K69jqObboszZdTKwD9WM5yf3Molo1Hcc8MeG8j8DZ9l+CbAG+EBZ/wFgTVl/VqlHOY+HAy8HDgK+Vq6vGJy5wC9svxR4JdW5yXXSQpImAx8BOsr0OGOofudzrbShUZPIUF3cy23fYftpqkku57Q4plHL9v22byzL66j+OE+m+s6/U6p9Bzi0LM8BznPlWmCCpB2AA4H5th+2vQaYT/UHIQZB0k7AW4FzS1nAfsAlpUr3c9J1ri4B9i/15wDfs/2U7TuB5VTXVwyQpK2BvwS+AWD7aduPkOtkJNiY6oncjYHNgfvJtdKWRlMiMxm4p6F8b1kXTVaaWfcCrgNe3DCP1irgxWW5t/OT8za0vgycADxXyi8CHrH9bCk3fr/Pf/dl+9pSP+dk6EwHVgPfKrf7zpU0nlwnLWV7JfAl4G6qBGYt0EmulbY0mhKZaAFJWwA/BP7B9gumWXf1bH+e7x8mkg4BHrDd2epY4nkbA7OAr9veC3ic/7mNBOQ6aYVyW24OVaK5IzCetHC1rdGUyKwEdm4o71TWRZNI2oQqifmu7UvL6v8sTeGUnw+U9b2dn5y3ofM64O2S7qK6tbofVf+MCaX5HF74/T7/3ZftWwMPkXMylO4F7rV9XSlfQpXY5DpprdnAnbZX234GuJTq+sm10oZGUyJzA7Bb6XW+KVUHrMtbHNOoVe4PfwP4o+0zGzZdDnQ9UXEUcFnD+veWpzL2BdaWpvVfAgdI2qb8L+mAsi4GyPanbe9kexrV7/+vbB8JXAW8o1Trfk66ztU7Sn2X9YeXJzWmU3U8vX6YPsaoYnsVcI+kvyir9gduJddJq90N7Ctp8/K3rOu85FppR7ZHzQs4GFgK3A6c2Op4RvMLeD1Vc/jNwKLyOpjqvvGVwDJgATCx1BfVU2W3A4upnhbo2tf7qTrJLQfe1+rPNhpewBuBn5blXaj+uC4HLgbGlvXjSnl52b5Lw/tPLOfqNuAtrf487fwCZgILy7XyY2CbXCetfwGnAn8ClgDnA2NzrbTnK1MURERERNsaTbeWIiIiYgOTRCYiIiLaVhKZiIiIaFtJZCIiIqJtJZGJiIiItpVEJiKaosz6/KGyvKOkS/p7z3oca6akg5u1/4gYuZLIRESzTAA+BGD7Ptvv6Lv6eplJNY5RRGxgkshERLOcAewqaZGkiyUtAZB0tKQfS5ov6S5JH5b0sTKp4rWSJpZ6u0r6haROSddIemlZ/05JSyTdJOnqMpL354DDyrEOkzRe0jclXV/2O6fh2JdJ+rWkZZJOLuvHS/pZ2ecSSYe15BuLiAHbuP8qERGD8ilgD9szywzpP23YtgfVjOnjqEZL/aTtvSSdBbyXahbvecCxtpdJ2gf4GtX8UScBB9peKWmC7aclnUQ1Cu6HASSdTjWM/PslTQCul7SgHHvvcvwngBsk/QyYCtxn+63l/Vs36TuJiCGWRCYiWuEq2+uAdZLWAj8p6xcDryizqr8WuLiaCgeohpAH+C3wbUk/oJrsrycHUE2geXwpjwOmlOX5th8CkHQp1XQbPwf+VdI/U03tcM1QfMiIaL4kMhHRCk81LD/XUH6O6u/SRsAjtmd2f6PtY0sLzVuBTkmv6mH/Av7G9m0vWFm9r/u8LLa9VNIsqn42n5d0pe3PDeJzRcQwSx+ZiGiWdcCWg3mj7UeBOyW9E6rZ1iW9sizvavs62ycBq4GdezjWL4G/LzMbI2mvhm1vljRR0mbAocBvJe0IPGH7AuCLwKzBxB0Rwy+JTEQ0Rbl989vSyfeLg9jFkcAHJN0E3ALMKeu/KGlx2e/vgJuAq4AZXZ19gdOATYCbJd1Syl2uB35INRv1D20vBPak6kezCDgZ+Pwg4o2IFsjs1xGxwZB0NA2dgiOi/aVFJiIiItpWWmQiIiKibaVFJiIiItpWEpmIiIhoW0lkIiIiom0lkYmIiIi2lUQmIiIi2tZ/Azsplj/Ye+dMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines import results_plotter\n",
    "\n",
    "# Helper from the library\n",
    "results_plotter.plot_results(['./logs/'], 1e5, results_plotter.X_TIMESTEPS, \"DDPG LunarLander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70452d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    print(x)\n",
    "    print(y)\n",
    "    y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6242350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  492  1117  1642 ... 98719 99257 99856]\n",
      "[-5 -4 -5 ... -5 -5 -5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results('./logs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2898c",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad9972e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "import time\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4167a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 608 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(eval_env, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%M') + '_PPO' +'/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e26684",
   "metadata": {},
   "source": [
    "### PPO2 algorithm directly trained against Built-in expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import PPO2\n",
    "env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model = PPO2('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb99b36",
   "metadata": {},
   "source": [
    "### Optimized PPO1 algorithm directly trained against Built-in expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SlimeVolley-v0')\n",
    "eval_env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=10000,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model_1 = PPO1('MlpPolicy', env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0\n",
    "               , optim_epochs=10,optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99\n",
    "               , lam=0.95, schedule='linear', verbose=2)\n",
    "model_1.learn(total_timesteps=1000000, callback=eval_callback)\n",
    "model_1.save(os.path.join('models', \"PPO_model\"))\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model_1, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928e63a",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a898287",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(eval_env, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%M') + '_PPO' +'/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action, _states = model_1.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269a5fc",
   "metadata": {},
   "source": [
    "## **A2C directly traind against Built-in expert againt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f665c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd8c704",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:194: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "---------------------------------------\n",
      "| EpLenMean          | 631            |\n",
      "| EpRewMean          | -4.69          |\n",
      "| EpThisIter         | 1              |\n",
      "| EpisodesSoFar      | 16             |\n",
      "| TimeElapsed        | 74.1           |\n",
      "| TimestepsSoFar     | 10240          |\n",
      "| ev_tdlam_before    | 0.888          |\n",
      "| explained_variance | -145           |\n",
      "| fps                | 13             |\n",
      "| loss_ent           | 1.9290515      |\n",
      "| loss_kl            | 3.0267984e-08  |\n",
      "| loss_pol_entpen    | -0.019290514   |\n",
      "| loss_pol_surr      | -4.6381727e-05 |\n",
      "| loss_vf_loss       | 0.005139376    |\n",
      "| nupdates           | 1              |\n",
      "| policy_entropy     | 2.08           |\n",
      "| total_timesteps    | 5              |\n",
      "| value_loss         | 0.0156         |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 574.80 +/- 106.51\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| explained_variance | -1.13e+05 |\n",
      "| fps                | 119       |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 2.08      |\n",
      "| total_timesteps    | 500       |\n",
      "| value_loss         | 0.0218    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 566.60 +/- 118.37\n",
      "---------------------------------\n",
      "| explained_variance | -106     |\n",
      "| fps                | 130      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 0.0102   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 584.20 +/- 74.25\n",
      "---------------------------------\n",
      "| explained_variance | -28.5    |\n",
      "| fps                | 135      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 0.00602  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 585.20 +/- 93.86\n",
      "---------------------------------\n",
      "| explained_variance | -252     |\n",
      "| fps                | 136      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 0.00621  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 558.80 +/- 100.22\n",
      "---------------------------------\n",
      "| explained_variance | -951     |\n",
      "| fps                | 137      |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 0.0129   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 492.40 +/- 86.21\n",
      "----------------------------------\n",
      "| explained_variance | -2.26e+03 |\n",
      "| fps                | 141       |\n",
      "| nupdates           | 600       |\n",
      "| policy_entropy     | 2.08      |\n",
      "| total_timesteps    | 3000      |\n",
      "| value_loss         | 0.0704    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 539.20 +/- 32.88\n",
      "---------------------------------\n",
      "| explained_variance | -336     |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 0.0147   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 600.80 +/- 99.72\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| explained_variance | -1.02e+03 |\n",
      "| fps                | 143       |\n",
      "| nupdates           | 800       |\n",
      "| policy_entropy     | 2.07      |\n",
      "| total_timesteps    | 4000      |\n",
      "| value_loss         | 0.0455    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 527.80 +/- 49.28\n",
      "---------------------------------\n",
      "| explained_variance | -469     |\n",
      "| fps                | 144      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 0.0108   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-4.60 +/- 0.49\n",
      "Episode length: 599.80 +/- 130.54\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| explained_variance | -186     |\n",
      "| fps                | 145      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 0.00087  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 611.40 +/- 121.94\n",
      "---------------------------------\n",
      "| explained_variance | -520     |\n",
      "| fps                | 143      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 0.0153   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-4.60 +/- 0.49\n",
      "Episode length: 576.60 +/- 57.32\n",
      "---------------------------------\n",
      "| explained_variance | -1.47    |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 0.000867 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 578.60 +/- 87.61\n",
      "---------------------------------\n",
      "| explained_variance | -76.2    |\n",
      "| fps                | 143      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 0.00365  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 614.60 +/- 71.94\n",
      "---------------------------------\n",
      "| explained_variance | -2.62    |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 0.00716  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 616.40 +/- 138.29\n",
      "---------------------------------\n",
      "| explained_variance | -3.83    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 0.000345 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 629.20 +/- 95.37\n",
      "---------------------------------\n",
      "| explained_variance | -42.2    |\n",
      "| fps                | 140      |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 0.000335 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 509.80 +/- 64.82\n",
      "---------------------------------\n",
      "| explained_variance | -14.3    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 0.00465  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 581.20 +/- 50.76\n",
      "---------------------------------\n",
      "| explained_variance | -54.2    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 9000     |\n",
      "| value_loss         | 0.00451  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=9500, episode_reward=-4.60 +/- 0.49\n",
      "Episode length: 605.00 +/- 80.07\n",
      "---------------------------------\n",
      "| explained_variance | 0.948    |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 2.03     |\n",
      "| total_timesteps    | 9500     |\n",
      "| value_loss         | 0.000208 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 625.00 +/- 85.11\n",
      "---------------------------------\n",
      "| explained_variance | -2.85    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 0.00386  |\n",
      "---------------------------------\n",
      "mean_reward:-4.82 +/- 0.48\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SlimeVolley-v0')\n",
    "eval_env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499b7a6",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24fa8a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 428 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(eval_env, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%M') + '_A2C' +'/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
