{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb530cb",
   "metadata": {},
   "source": [
    "# **Code for first presentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5335ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "from gym import wrappers\n",
    "import time\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "env_to_wrap = gym.make(\"SlimeVolley-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc256af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 738 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(env_to_wrap, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%S') + '/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00515ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'UP', 'RIGHT', 'LEFT', 'UPRIGHT', 'UPLEFT']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c9cda",
   "metadata": {},
   "source": [
    "# **Second part of the project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661f1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "#python version must be 3.7\n",
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375e1a66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines[mpi]==2.10.2\n",
      "  Using cached stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.1 MB 157 kB/s eta 0:00:01     |████████▋                       | 10.2 MB 1.2 MB/s eta 0:00:23\n",
      "\u001b[?25hCollecting cloudpickle>=0.5.5\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: numpy in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from stable-baselines[mpi]==2.10.2) (1.21.4)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.3 MB 48 kB/s  eta 0:00:01     |████████▍                       | 15.8 MB 2.9 MB/s eta 0:00:16     |█████████                       | 17.0 MB 1.5 MB/s eta 0:00:29     |██████████████████▏             | 34.1 MB 1.9 MB/s eta 0:00:14     |████████████████████████████    | 52.9 MB 3.0 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting gym[atari,classic_control]>=0.11\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 2.4 MB/s eta 0:00:01    |██████████████▎                 | 5.0 MB 3.3 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting mpi4py\n",
      "  Using cached mpi4py-3.1.3.tar.gz (2.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata>=4.8.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (4.8.2)\n",
      "Collecting ale-py~=0.7.1\n",
      "  Downloading ale_py-0.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 832 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyglet>=1.4.0\n",
      "  Using cached pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib_metadata>=4.8.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib_metadata>=4.8.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.2) (3.6.0)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (21.3)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting setuptools-scm>=4\n",
      "  Using cached setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.10.2) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines[mpi]==2.10.2) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->stable-baselines[mpi]==2.10.2) (58.0.4)\n",
      "Collecting tomli>=1.0.0\n",
      "  Using cached tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Building wheels for collected packages: gym, mpi4py\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=4700d7fe09f8fa31106aa3f0d7c622102c31ae8babf120a8908468def7dd9165\n",
      "  Stored in directory: /home/amirmahdi/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
      "  Building wheel for mpi4py (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=3121081 sha256=586bf75605a1632b0649892246f6d02ad78c2149536fc0f40f9118d23bb37be7\n",
      "  Stored in directory: /home/amirmahdi/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n",
      "Successfully built gym mpi4py\n",
      "Installing collected packages: tomli, importlib-resources, cloudpickle, setuptools-scm, pytz, pyglet, pillow, kiwisolver, gym, fonttools, cycler, ale-py, scipy, pandas, opencv-python, matplotlib, joblib, stable-baselines, mpi4py\n",
      "Successfully installed ale-py-0.7.3 cloudpickle-2.0.0 cycler-0.11.0 fonttools-4.28.2 gym-0.21.0 importlib-resources-5.4.0 joblib-1.1.0 kiwisolver-1.3.2 matplotlib-3.5.0 mpi4py-3.1.3 opencv-python-4.5.4.60 pandas-1.3.4 pillow-8.4.0 pyglet-1.5.21 pytz-2021.3 scipy-1.7.3 setuptools-scm-6.3.2 stable-baselines-2.10.2 tomli-1.2.2\n",
      "Collecting slimevolleygym\n",
      "  Using cached slimevolleygym-0.1.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from slimevolleygym) (1.21.4)\n",
      "Requirement already satisfied: gym>=0.9.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from slimevolleygym) (0.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym>=0.9.4->slimevolleygym) (4.8.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from gym>=0.9.4->slimevolleygym) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym>=0.9.4->slimevolleygym) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym>=0.9.4->slimevolleygym) (3.6.0)\n",
      "Installing collected packages: slimevolleygym\n",
      "Successfully installed slimevolleygym-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines[mpi]==2.10.2\n",
    "!pip install slimevolleygym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed993e",
   "metadata": {},
   "source": [
    "### PPO1 algorithm directly trained against Built-in expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed472aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "from stable_baselines import PPO1\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.bench import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad4e996",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29501/3240459148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SlimeVolley-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SlimeVolley-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_dir' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('SlimeVolley-v0')\n",
    "env = Monitor(env, log_dir)\n",
    "eval_env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model = PPO1('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2898c",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad9972e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "import time\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4167a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 608 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(eval_env, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%M') + '_PPO' +'/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399a317",
   "metadata": {},
   "source": [
    "### PPO2 algorithm directly trained against Built-in expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb70502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import PPO2\n",
    "env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model = PPO2('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea9983",
   "metadata": {},
   "source": [
    "### Optimized PPO1 algorithm directly trained against Built-in expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62887ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SlimeVolley-v0')\n",
    "eval_env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=10000,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model_1 = PPO1('MlpPolicy', env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0\n",
    "               , optim_epochs=10,optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99\n",
    "               , lam=0.95, schedule='linear', verbose=2)\n",
    "model_1.learn(total_timesteps=1000000, callback=eval_callback)\n",
    "model_1.save(os.path.join('models', \"PPO_model\"))\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model_1, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426ca44",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(eval_env, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%M') + '_PPO' +'/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action, _states = model_1.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269a5fc",
   "metadata": {},
   "source": [
    "## **A2C directly traind against Built-in expert againt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f665c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd8c704",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/amirmahdi/anaconda3/envs/RL_Project2/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:194: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "---------------------------------------\n",
      "| EpLenMean          | 631            |\n",
      "| EpRewMean          | -4.69          |\n",
      "| EpThisIter         | 1              |\n",
      "| EpisodesSoFar      | 16             |\n",
      "| TimeElapsed        | 74.1           |\n",
      "| TimestepsSoFar     | 10240          |\n",
      "| ev_tdlam_before    | 0.888          |\n",
      "| explained_variance | -145           |\n",
      "| fps                | 13             |\n",
      "| loss_ent           | 1.9290515      |\n",
      "| loss_kl            | 3.0267984e-08  |\n",
      "| loss_pol_entpen    | -0.019290514   |\n",
      "| loss_pol_surr      | -4.6381727e-05 |\n",
      "| loss_vf_loss       | 0.005139376    |\n",
      "| nupdates           | 1              |\n",
      "| policy_entropy     | 2.08           |\n",
      "| total_timesteps    | 5              |\n",
      "| value_loss         | 0.0156         |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 574.80 +/- 106.51\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| explained_variance | -1.13e+05 |\n",
      "| fps                | 119       |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 2.08      |\n",
      "| total_timesteps    | 500       |\n",
      "| value_loss         | 0.0218    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 566.60 +/- 118.37\n",
      "---------------------------------\n",
      "| explained_variance | -106     |\n",
      "| fps                | 130      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 0.0102   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 584.20 +/- 74.25\n",
      "---------------------------------\n",
      "| explained_variance | -28.5    |\n",
      "| fps                | 135      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 0.00602  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 585.20 +/- 93.86\n",
      "---------------------------------\n",
      "| explained_variance | -252     |\n",
      "| fps                | 136      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 0.00621  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 558.80 +/- 100.22\n",
      "---------------------------------\n",
      "| explained_variance | -951     |\n",
      "| fps                | 137      |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 0.0129   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 492.40 +/- 86.21\n",
      "----------------------------------\n",
      "| explained_variance | -2.26e+03 |\n",
      "| fps                | 141       |\n",
      "| nupdates           | 600       |\n",
      "| policy_entropy     | 2.08      |\n",
      "| total_timesteps    | 3000      |\n",
      "| value_loss         | 0.0704    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 539.20 +/- 32.88\n",
      "---------------------------------\n",
      "| explained_variance | -336     |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 0.0147   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 600.80 +/- 99.72\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| explained_variance | -1.02e+03 |\n",
      "| fps                | 143       |\n",
      "| nupdates           | 800       |\n",
      "| policy_entropy     | 2.07      |\n",
      "| total_timesteps    | 4000      |\n",
      "| value_loss         | 0.0455    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 527.80 +/- 49.28\n",
      "---------------------------------\n",
      "| explained_variance | -469     |\n",
      "| fps                | 144      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 0.0108   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-4.60 +/- 0.49\n",
      "Episode length: 599.80 +/- 130.54\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| explained_variance | -186     |\n",
      "| fps                | 145      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 2.08     |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 0.00087  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 611.40 +/- 121.94\n",
      "---------------------------------\n",
      "| explained_variance | -520     |\n",
      "| fps                | 143      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 0.0153   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-4.60 +/- 0.49\n",
      "Episode length: 576.60 +/- 57.32\n",
      "---------------------------------\n",
      "| explained_variance | -1.47    |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 0.000867 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 578.60 +/- 87.61\n",
      "---------------------------------\n",
      "| explained_variance | -76.2    |\n",
      "| fps                | 143      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 0.00365  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 614.60 +/- 71.94\n",
      "---------------------------------\n",
      "| explained_variance | -2.62    |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 0.00716  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 616.40 +/- 138.29\n",
      "---------------------------------\n",
      "| explained_variance | -3.83    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 0.000345 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 629.20 +/- 95.37\n",
      "---------------------------------\n",
      "| explained_variance | -42.2    |\n",
      "| fps                | 140      |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 0.000335 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 509.80 +/- 64.82\n",
      "---------------------------------\n",
      "| explained_variance | -14.3    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 0.00465  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 581.20 +/- 50.76\n",
      "---------------------------------\n",
      "| explained_variance | -54.2    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 2.07     |\n",
      "| total_timesteps    | 9000     |\n",
      "| value_loss         | 0.00451  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=9500, episode_reward=-4.60 +/- 0.49\n",
      "Episode length: 605.00 +/- 80.07\n",
      "---------------------------------\n",
      "| explained_variance | 0.948    |\n",
      "| fps                | 142      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 2.03     |\n",
      "| total_timesteps    | 9500     |\n",
      "| value_loss         | 0.000208 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-4.80 +/- 0.40\n",
      "Episode length: 625.00 +/- 85.11\n",
      "---------------------------------\n",
      "| explained_variance | -2.85    |\n",
      "| fps                | 141      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 2.06     |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 0.00386  |\n",
      "---------------------------------\n",
      "mean_reward:-4.82 +/- 0.48\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SlimeVolley-v0')\n",
    "eval_env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499b7a6",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24fa8a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 428 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(eval_env, './videos/SlimeVolley' + time.strftime('%Y_%m_%d_%H_%M') + '_A2C' +'/', force = True)\n",
    "state = env.reset()\n",
    "t=0\n",
    "while True:\n",
    "    t+=1\n",
    "    env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665b854",
   "metadata": {},
   "source": [
    "### Note:\n",
    "I have used the stabebaseline built-in ploter from the link below:\n",
    "https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/monitor_training.ipynb#scrollTo=BIedd7Pz9sOs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
